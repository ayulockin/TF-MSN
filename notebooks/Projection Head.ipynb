{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971e5f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from configs.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aefaf8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "callback_config:\n",
       "  checkpoint_filepath: wandb/model_{epoch}\n",
       "  early_patience: 6\n",
       "  rlrp_factor: 0.2\n",
       "  rlrp_patience: 3\n",
       "  save_best_only: true\n",
       "  use_earlystopping: true\n",
       "  use_reduce_lr_on_plateau: false\n",
       "  viz_num_images: 100\n",
       "dataset_config:\n",
       "  batch_size: 8\n",
       "  num_crops:\n",
       "  - 2\n",
       "  - 5\n",
       "  shuffle_buffer: 100\n",
       "  size_crops:\n",
       "  - 224\n",
       "  - 96\n",
       "  use_options: true\n",
       "model_config:\n",
       "  anchor_tau: 0.01\n",
       "  attention_probs_dropout_prob: 0.0\n",
       "  backbone: ViT\n",
       "  dropout_rate: 0.5\n",
       "  hidden_act: gelu\n",
       "  hidden_dropout_prob: 0.0\n",
       "  hidden_size: 768\n",
       "  initializer_range: 0.02\n",
       "  intermediate_size: 3072\n",
       "  is_encoder_decoder: false\n",
       "  layer_norm_eps: 1.0e-12\n",
       "  mask_ratio: 0.75\n",
       "  model_img_channels: 3\n",
       "  model_img_height: 224\n",
       "  model_img_width: 224\n",
       "  norm_pix_loss: false\n",
       "  num_attention_heads: 12\n",
       "  num_channels: 3\n",
       "  num_hidden_layers: 12\n",
       "  num_prototypes: 10\n",
       "  patch_size: 16\n",
       "  post_gap_dropout: false\n",
       "  proj_hidden_size: 1024\n",
       "  proj_output_dim: 256\n",
       "  proj_use_bn: true\n",
       "  qkv_bias: true\n",
       "  target_tau: 0.1\n",
       "seed: 0\n",
       "train_config:\n",
       "  epochs: 3\n",
       "  loss: categorical_crossentropy\n",
       "  metrics:\n",
       "  - accuracy\n",
       "  optimizer: adam\n",
       "  sgd_momentum: 0.9\n",
       "  use_augmentations: false\n",
       "  use_class_weights: false\n",
       "wandb_config:\n",
       "  project: tf-msn"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = get_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c046af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from transformers.models.vit_mae.configuration_vit_mae import ViTMAEConfig\n",
    "\n",
    "from msn.model.encoder import ViTMAEEmbeddings\n",
    "from msn.model.encoder import TFViTMAEEncoder\n",
    "from msn.model.projection import ProjectionHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fbe8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vit_mae_configs(args):\n",
    "    custom_config = ViTMAEConfig(\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_hidden_layers=args.num_hidden_layers,\n",
    "        num_attention_heads=args.num_attention_heads,\n",
    "        intermediate_size=args.intermediate_size,\n",
    "        hidden_act=args.hidden_act,\n",
    "        hidden_dropout_prob=args.hidden_dropout_prob,\n",
    "        attention_probs_dropout_prob=args.attention_probs_dropout_prob,\n",
    "        initializer_range=args.initializer_range,\n",
    "        layer_norm_eps=args.layer_norm_eps,\n",
    "        is_encoder_decoder=args.is_encoder_decoder,\n",
    "        image_size=(args.model_img_width, args.model_img_height),\n",
    "        patch_size=(args.patch_size, args.patch_size),\n",
    "        num_channels=args.model_img_channels,\n",
    "        qkv_bias=args.qkv_bias,\n",
    "        mask_ratio=args.mask_ratio,\n",
    "        norm_pix_loss=args.norm_pix_loss,\n",
    "        proj_hidden_size = args.proj_hidden_size,\n",
    "        proj_output_dim = args.proj_output_dim,\n",
    "        proj_use_bn = args.proj_use_bn\n",
    "    )\n",
    "\n",
    "    return custom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3594353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTMAEConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"decoder_hidden_size\": 512,\n",
       "  \"decoder_intermediate_size\": 2048,\n",
       "  \"decoder_num_attention_heads\": 16,\n",
       "  \"decoder_num_hidden_layers\": 8,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": [\n",
       "    224,\n",
       "    224\n",
       "  ],\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"mask_ratio\": 0.75,\n",
       "  \"model_type\": \"vit_mae\",\n",
       "  \"norm_pix_loss\": false,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": [\n",
       "    16,\n",
       "    16\n",
       "  ],\n",
       "  \"proj_hidden_size\": 1024,\n",
       "  \"proj_output_dim\": 256,\n",
       "  \"proj_use_bn\": true,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.23.1\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = config.model_config\n",
    "custom_config = get_vit_mae_configs(args)\n",
    "custom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116f299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFMAEViTModelWithProjection(tf.keras.Model):\n",
    "    \"\"\"The encoder model with projection head.\"\"\"\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.config = get_vit_mae_configs(config)\n",
    "\n",
    "        self.embeddings = ViTMAEEmbeddings(self.config, name=\"embeddings\")\n",
    "        self.encoder = TFViTMAEEncoder(self.config, name=\"encoder\")\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=self.config.layer_norm_eps, name=\"layernorm\"\n",
    "        )\n",
    "        self.projection_head = ProjectionHead(self.config, name=\"projection_head\")\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        pixel_values = None,\n",
    "        noise: tf.Tensor = None,\n",
    "        head_mask = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict = None,\n",
    "        training: bool = False,\n",
    "    ) -> tf.Tensor:\n",
    "        embedding_output, mask, ids_restore = self.embeddings(\n",
    "            pixel_values=pixel_values, training=training, noise=noise\n",
    "        )\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        if head_mask is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            head_mask = [None] * self.config.num_hidden_layers\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            training=training,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        sequence_output = self.layernorm(inputs=sequence_output)\n",
    "        \n",
    "        # Pass the [CLS] token to the projection head\n",
    "        projection_output = self.projection_head(sequence_output[:, 0, :])\n",
    "\n",
    "        return projection_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e8156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = TFMAEViTModelWithProjection(config.model_config)\n",
    "target = TFMAEViTModelWithProjection(config.model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3de749eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msn.utils import build_and_clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790b5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor, target = build_and_clone_model(anchor, target, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34961148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfmae_vi_t_model_with_projection_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embeddings (ViTMAEEmbedding  multiple                 742656    \n",
      " s)                                                              \n",
      "                                                                 \n",
      " encoder (TFViTMAEEncoder)   multiple                  85054464  \n",
      "                                                                 \n",
      " layernorm (LayerNormalizati  multiple                 1536      \n",
      " on)                                                             \n",
      "                                                                 \n",
      " projection_head (Projection  multiple                 2107648   \n",
      " Head)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,906,304\n",
      "Trainable params: 87,750,912\n",
      "Non-trainable params: 155,392\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "anchor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae7970e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.08931544 -0.27112478  0.02645697 ...  0.3183299  -0.06450961\n",
      "  -0.6526876 ]\n",
      " [-0.19172618 -0.18916884  0.02150304 ...  0.24803305  0.03818919\n",
      "  -0.521096  ]\n",
      " [-0.12475991 -0.11191332 -0.08389263 ...  0.5131528   0.1558488\n",
      "  -0.44654652]\n",
      " ...\n",
      " [-0.16175449 -0.1768989   0.03484544 ...  0.34487978  0.11445557\n",
      "  -0.5612816 ]\n",
      " [-0.1758292  -0.19360761 -0.02152263 ...  0.3325669   0.07130273\n",
      "  -0.5162564 ]\n",
      " [-0.14243533 -0.23717216 -0.05518706 ...  0.40793493 -0.01335798\n",
      "  -0.55936456]], shape=(8, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pixel_values = tf.random.normal((8, 224, 224, 3))\n",
    "print(anchor(pixel_values=pixel_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305d7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
